{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bengali_automated_notes_writing_service.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO/EMRFeRk7vMve7QxJ/53D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Muhaiminul8473/Bengali-automated-notes-generation-service/blob/main/Bengali_automated_notes_writing_service.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6ybrlWwv53O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9daeef84-de8a-4e0e-83ed-466b2a330c66"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMwkdQ-oyzOt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "014b02b8-fd73-456d-c097-696b709048ed"
      },
      "source": [
        "pip install tensorflow==1.13.2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.13.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/d3/651f95288a6cd9094f7411cdd90ef12a3d01a268009e0e3cd66b5c8d65bd/tensorflow-1.13.2-cp36-cp36m-manylinux1_x86_64.whl (92.6MB)\n",
            "\u001b[K     |████████████████████████████████| 92.6MB 55kB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (0.36.2)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 50.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (3.12.4)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.19.4)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.32.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (0.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.15.0)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.8MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 38.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (0.10.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (0.3.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (1.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.2) (50.3.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.2) (2.10.0)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/03/b7e605db4a57c0f6fba744b11ef3ddf4ddebcada35022927a2b5fc623fdf/mock-4.0.3-py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.4.0)\n",
            "Installing collected packages: tensorboard, keras-applications, mock, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.4.0\n",
            "    Uninstalling tensorboard-2.4.0:\n",
            "      Successfully uninstalled tensorboard-2.4.0\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorflow 2.4.0\n",
            "    Uninstalling tensorflow-2.4.0:\n",
            "      Successfully uninstalled tensorflow-2.4.0\n",
            "Successfully installed keras-applications-1.0.8 mock-4.0.3 tensorboard-1.13.1 tensorflow-1.13.2 tensorflow-estimator-1.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Lyy5QORwa4k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "702cb97f-6455-4e2a-d03d-ecea35603fa3"
      },
      "source": [
        "pip install tflearn"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tflearn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/3c/0b156d08ef3d4e2a8009ecab2af1ad2e304f6fb99562b6271c68a74a4397/tflearn-0.5.0.tar.gz (107kB)\n",
            "\r\u001b[K     |███                             | 10kB 17.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 20kB 20.6MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 30kB 12.4MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 40kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 51kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 61kB 11.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 71kB 11.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 81kB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 92kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 102kB 11.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 11.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tflearn) (1.19.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tflearn) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from tflearn) (7.0.0)\n",
            "Building wheels for collected packages: tflearn\n",
            "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflearn: filename=tflearn-0.5.0-cp36-none-any.whl size=127301 sha256=59b4c90f8612a6a94ef5726e4655f48ed470410943441e1dcab1dfae874aea4a\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/d2/ed/fb9a0d301dd9586c11e9547120278e624227f22fd5f4baf744\n",
            "Successfully built tflearn\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbSA2nniwHJ3"
      },
      "source": [
        "#Used in Tensorflow Model\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tflearn\n",
        "import random\n",
        "\n",
        "#Usde to for Contextualisation and Other NLP Tasks.\n",
        "import nltk\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "#Other\n",
        "import json\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvVp-xyU7s16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f33476f-9b6d-4368-b83e-a12ee300d12a"
      },
      "source": [
        "nltk.download('punkt')\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "narKWWgMyFRt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a0012d1-4835-42e6-adf7-289427c0f86b"
      },
      "source": [
        "print(tf.__version__)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.13.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XI1MUCMNwK59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b964e66-8481-4c84-f3ce-eb8b56a0cbdd"
      },
      "source": [
        "print(\"Processing the Intents.....\")\n",
        "with open('/content/gdrive/My Drive/Final defence/intents2.json') as json_data:\n",
        "    intents = json.load(json_data)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing the Intents.....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDmz-JXVwQv9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "313e542c-7e2e-4c11-bfe2-1950d210deb7"
      },
      "source": [
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?']\n",
        "print(\"Looping through the Intents to Convert them to words, classes, documents and ignore_words.......\")\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        # tokenize each word in the sentence\n",
        "        w = nltk.word_tokenize(pattern)\n",
        "        # add to our words list\n",
        "        words.extend(w)\n",
        "        # add to documents in our corpus\n",
        "        documents.append((w, intent['tag']))\n",
        "        # add to our classes list\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looping through the Intents to Convert them to words, classes, documents and ignore_words.......\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tStKFINWGSb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d7e6596-9228-49b0-92cb-dbaeedaf6223"
      },
      "source": [
        "print(words)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['শাপলা', 'চত্বরে', 'বাসের', 'ধাক্কায়', 'নিহত', 'চত্বরে', 'বাসের', 'ধাক্কায়', 'নিহত', 'শাপলা', 'চত্বরে', 'বাসের', 'ধাক্কায়', 'অনুমতি', 'না', 'পেলেও', 'সমাবেশ', 'করবে', 'বিএনপি', 'অনুমতি', 'না', 'পেলেও', 'সমাবেশ', 'করবে', 'সমাবেশ', 'করবে', 'বিএনপি', 'ইসলামবাগে', 'প্লাস্টিক', 'কারখানায়', 'আগুন', 'প্লাস্টিক', 'কারখানায়', 'আগুন', 'ইসলামবাগে', 'প্লাস্টিক', 'কারখানায়', 'ফকিরাপুলে', 'আবাসিক', 'হোটেলে', 'খুন', 'আবাসিক', 'হোটেলে', 'খুন', 'ফকিরাপুলে', 'আবাসিক', 'হোটেলে', 'শনিবারের', 'মধ্যে', 'অবৈধ', 'বিলবোর্ড', 'সরাতে', 'পুলিশের', 'নির্দেশ', 'অবৈধ', 'বিলবোর্ড', 'সরাতে', 'পুলিশের', 'নির্দেশ', 'শনিবারের', 'মধ্যে', 'অবৈধ', 'বিলবোর্ড', 'নির্বাচন', 'কমিশন', 'ছাড়া', 'কারও', 'সঙ্গে', 'সংলাপ', 'নয়', ':', 'নাসিম', 'নির্বাচন', 'কমিশন', 'ছাড়া', 'কারও', 'সঙ্গে', 'সংলাপ', 'নয়', 'কারও', 'সঙ্গে', 'সংলাপ', 'নয়', ':', 'নাসিম', 'জামালপুরে', 'বিদ্যুৎস্পৃষ্ট', 'হয়ে', 'কলেজছাত্রীর', 'মৃত্যু', 'কলেজছাত্রীর', 'মৃত্যু', 'জামালপুরে', 'বিদ্যুৎস্পৃষ্ট', 'হয়ে', 'ঢাকা-চট্টগ্রাম', 'ও', 'ঢাকা-ময়মনসিংহ', 'মহাসড়কের', 'কাজ', 'জুনে', 'শেষ', 'হবে', 'ঢাকা-চট্টগ্রাম', 'ও', 'ঢাকা-ময়মনসিংহ', 'মহাসড়কের', 'কাজ', 'মহাসড়কের', 'কাজ', 'জুনে', 'শেষ', 'হবে', 'চুরি', 'হওয়া', 'নবজাতক', 'ফিরে', 'গেল', 'মায়ের', 'কোলে', 'ফিরে', 'গেল', 'মায়ের', 'কোলে', 'চুরি', 'হওয়া', 'নবজাতক']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTLJZVBVWxqn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fdd8e12-c096-432d-faa9-31d788322c89"
      },
      "source": [
        "print(documents)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(['শাপলা', 'চত্বরে', 'বাসের', 'ধাক্কায়', 'নিহত'], 'নিহত'), (['চত্বরে', 'বাসের', 'ধাক্কায়', 'নিহত'], 'নিহত'), (['শাপলা', 'চত্বরে', 'বাসের', 'ধাক্কায়'], 'নিহত'), (['অনুমতি', 'না', 'পেলেও', 'সমাবেশ', 'করবে', 'বিএনপি'], 'বিএনপি'), (['অনুমতি', 'না', 'পেলেও', 'সমাবেশ', 'করবে'], 'বিএনপি'), (['সমাবেশ', 'করবে', 'বিএনপি'], 'বিএনপি'), (['ইসলামবাগে', 'প্লাস্টিক', 'কারখানায়', 'আগুন'], 'আগুন'), (['প্লাস্টিক', 'কারখানায়', 'আগুন'], 'আগুন'), (['ইসলামবাগে', 'প্লাস্টিক', 'কারখানায়'], 'আগুন'), (['ফকিরাপুলে', 'আবাসিক', 'হোটেলে', 'খুন'], 'খুন'), (['আবাসিক', 'হোটেলে', 'খুন'], 'খুন'), (['ফকিরাপুলে', 'আবাসিক', 'হোটেলে'], 'খুন'), (['শনিবারের', 'মধ্যে', 'অবৈধ', 'বিলবোর্ড', 'সরাতে', 'পুলিশের', 'নির্দেশ'], 'বিলবোর্ড'), (['অবৈধ', 'বিলবোর্ড', 'সরাতে', 'পুলিশের', 'নির্দেশ'], 'বিলবোর্ড'), (['শনিবারের', 'মধ্যে', 'অবৈধ', 'বিলবোর্ড'], 'বিলবোর্ড'), (['নির্বাচন', 'কমিশন', 'ছাড়া', 'কারও', 'সঙ্গে', 'সংলাপ', 'নয়', ':', 'নাসিম'], 'নির্বাচন'), (['নির্বাচন', 'কমিশন', 'ছাড়া', 'কারও', 'সঙ্গে', 'সংলাপ', 'নয়'], 'নির্বাচন'), (['কারও', 'সঙ্গে', 'সংলাপ', 'নয়', ':', 'নাসিম'], 'নির্বাচন'), (['জামালপুরে', 'বিদ্যুৎস্পৃষ্ট', 'হয়ে', 'কলেজছাত্রীর', 'মৃত্যু'], 'বিদ্যুৎস্পৃষ্ট'), (['কলেজছাত্রীর', 'মৃত্যু'], 'বিদ্যুৎস্পৃষ্ট'), (['জামালপুরে', 'বিদ্যুৎস্পৃষ্ট', 'হয়ে'], 'বিদ্যুৎস্পৃষ্ট'), (['ঢাকা-চট্টগ্রাম', 'ও', 'ঢাকা-ময়মনসিংহ', 'মহাসড়কের', 'কাজ', 'জুনে', 'শেষ', 'হবে'], 'মহাসড়কের'), (['ঢাকা-চট্টগ্রাম', 'ও', 'ঢাকা-ময়মনসিংহ', 'মহাসড়কের', 'কাজ'], 'মহাসড়কের'), (['মহাসড়কের', 'কাজ', 'জুনে', 'শেষ', 'হবে'], 'মহাসড়কের'), (['চুরি', 'হওয়া', 'নবজাতক', 'ফিরে', 'গেল', 'মায়ের', 'কোলে'], 'নবজাতক'), (['ফিরে', 'গেল', 'মায়ের', 'কোলে'], 'নবজাতক'), (['চুরি', 'হওয়া', 'নবজাতক'], 'নবজাতক')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tX2Z9Xt9wUUu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "481f571d-c5b8-4ee1-8bc3-708e35bf714f"
      },
      "source": [
        "print(\"Stemming, Lowering and Removing Duplicates.......\")\n",
        "words = [stemmer.stem(w) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "# remove duplicates\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "print (len(documents), \"documents\")\n",
        "print (len(classes), \"classes\", classes)\n",
        "print (len(words), \"unique stemmed words\", words)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stemming, Lowering and Removing Duplicates.......\n",
            "27 documents\n",
            "9 classes ['আগুন', 'খুন', 'নবজাতক', 'নির্বাচন', 'নিহত', 'বিএনপি', 'বিদ্যুৎস্পৃষ্ট', 'বিলবোর্ড', 'মহাসড়কের']\n",
            "55 unique stemmed words [':', 'অনুমতি', 'অবৈধ', 'আগুন', 'আবাসিক', 'ইসলামবাগে', 'ও', 'কমিশন', 'করবে', 'কলেজছাত্রীর', 'কাজ', 'কারও', 'কারখানায়', 'কোলে', 'খুন', 'গেল', 'চত্বরে', 'চুরি', 'ছাড়া', 'জামালপুরে', 'জুনে', 'ঢাকা-চট্টগ্রাম', 'ঢাকা-ময়মনসিংহ', 'ধাক্কায়', 'নবজাতক', 'নয়', 'না', 'নাসিম', 'নির্দেশ', 'নির্বাচন', 'নিহত', 'পুলিশের', 'পেলেও', 'প্লাস্টিক', 'ফকিরাপুলে', 'ফিরে', 'বাসের', 'বিএনপি', 'বিদ্যুৎস্পৃষ্ট', 'বিলবোর্ড', 'মধ্যে', 'মহাসড়কের', 'মায়ের', 'মৃত্যু', 'শনিবারের', 'শাপলা', 'শেষ', 'সংলাপ', 'সঙ্গে', 'সমাবেশ', 'সরাতে', 'হওয়া', 'হবে', 'হয়ে', 'হোটেলে']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN8Yb0RwghrS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eb682a0-867c-4aed-f63f-3809bbf696f4"
      },
      "source": [
        "print(len(words))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVCDSUoVwVCl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9df78ef0-581b-4ef1-c84d-086652ffc38e"
      },
      "source": [
        "print(\"Creating the Data for our Model.....\")\n",
        "training = []\n",
        "output = []\n",
        "print(\"Creating an List (Empty) for Output.....\")\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "print(\"Creating Traning Set, Bag of Words for our Model....\")\n",
        "for doc in documents:\n",
        "    # initialize our bag of words\n",
        "    bag = []\n",
        "    # list of tokenized words for the pattern\n",
        "    pattern_words = doc[0]\n",
        "    # stem each word\n",
        "    pattern_words = [stemmer.stem(word) for word in pattern_words]\n",
        "    # create our bag of words array\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "\n",
        "    # output is a '0' for each tag and '1' for current tag\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "    training.append([bag, output_row])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating the Data for our Model.....\n",
            "Creating an List (Empty) for Output.....\n",
            "Creating Traning Set, Bag of Words for our Model....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggNzZABBe1KL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fb013ab-3ee6-4a89-b7f1-4315d740030e"
      },
      "source": [
        "print(len(words))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUgs1U6zaRWf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d69352d4-dcb1-4bb9-df0b-99a6dcd91ae7"
      },
      "source": [
        "print(len(pattern_words))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGeV77U2ilag",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f5c6bc5-f708-4087-9ba3-c131605b2ec6"
      },
      "source": [
        "print(len(bag))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78OhBOePwXre",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcacaf45-ce04-4e1d-9978-7aaeac22cdf3"
      },
      "source": [
        "print(\"Shuffling Randomly and Converting into Numpy Array for Faster Processing......\")\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "print(\"Creating Train and Test Lists.....\")\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])\n",
        "print(\"Building Neural Network for Out Chatbot to be Contextual....\")\n",
        "print(\"Resetting graph data....\")\n",
        "tf.reset_default_graph()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shuffling Randomly and Converting into Numpy Array for Faster Processing......\n",
            "Creating Train and Test Lists.....\n",
            "Building Neural Network for Out Chatbot to be Contextual....\n",
            "Resetting graph data....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKErwm9ywaW9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ac2b7b1-c0b5-44c0-8a29-bed07400ebb8"
      },
      "source": [
        "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
        "net = tflearn.regression(net)\n",
        "print(\"Training....\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUyd0XLMwc8O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa3a81b3-7535-470d-9280-6b772b1b7c08"
      },
      "source": [
        "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_ius7F8wfN5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61abe501-de87-422a-a1d4-4d4e909cc638"
      },
      "source": [
        "print(\"Training the Model.......\")\n",
        "model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)\n",
        "print(\"Saving the Model.......\")\n",
        "model.save('model.tflearn')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Step: 3999  | total loss: \u001b[1m\u001b[32m0.00290\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 1000 | loss: 0.00290 - acc: 1.0000 -- iter: 24/27\n",
            "Training Step: 4000  | total loss: \u001b[1m\u001b[32m0.00294\u001b[0m\u001b[0m | time: 0.032s\n",
            "| Adam | epoch: 1000 | loss: 0.00294 - acc: 1.0000 -- iter: 27/27\n",
            "--\n",
            "Saving the Model.......\n",
            "INFO:tensorflow:/content/model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BnvRTFOwhcH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e71ab95-0ace-456f-c74f-25abd7c8c9c9"
      },
      "source": [
        "print(\"Pickle is also Saved..........\")\n",
        "pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( \"training_data\", \"wb\" ) )"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pickle is also Saved..........\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id7Kzc3Rwj53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7507ca79-03ae-4b6e-fa2b-60a4ddd8bb5b"
      },
      "source": [
        "print(\"Loading Pickle.....\")\n",
        "data = pickle.load( open( \"training_data\", \"rb\" ) )\n",
        "words = data['words']\n",
        "classes = data['classes']\n",
        "train_x = data['train_x']\n",
        "train_y = data['train_y']\n",
        "\n",
        "\n",
        "with open('/content/gdrive/My Drive/Final defence/intents2.json') as json_data:\n",
        "    intents = json.load(json_data)\n",
        "    \n",
        "print(\"Loading the Model......\")\n",
        "# load our saved model\n",
        "model.load('./model.tflearn')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Pickle.....\n",
            "Loading the Model......\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from /content/model.tflearn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqxIOYJiwmXd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31164ecc-2a35-4972-9ab5-5a9fd275215b"
      },
      "source": [
        "def clean_up_sentence(sentence):\n",
        "    # It Tokenize or Break it into the constituents parts of Sentense.\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    # Stemming means to find the root of the word.\n",
        "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
        "    return sentence_words\n",
        "\n",
        "# Return the Array of Bag of Words: True or False and 0 or 1 for each word of bag that exists in the Sentence\n",
        "def bow(sentence, words, show_details=False):\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    bag = [0]*len(words)\n",
        "    for s in sentence_words:\n",
        "        for i,w in enumerate(words):\n",
        "            if w == s:\n",
        "                bag[i] = 1\n",
        "                if show_details:\n",
        "                    print (\"found in bag: %s\" % w)\n",
        "    return(np.array(bag))\n",
        "\n",
        "ERROR_THRESHOLD = 0.25\n",
        "print(\"ERROR_THRESHOLD = 0.25\")\n",
        "\n",
        "def classify(sentence):\n",
        "    # Prediction or To Get the Posibility or Probability from the Model\n",
        "    results = model.predict([bow(sentence, words)])[0]\n",
        "    # Exclude those results which are Below Threshold\n",
        "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
        "    # Sorting is Done because heigher Confidence Answer comes first.\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append((classes[r[0]], r[1])) #Tuppl -> Intent and Probability\n",
        "    return return_list\n",
        "\n",
        "def response(sentence, userID='123', show_details=False):\n",
        "    results = classify(sentence)\n",
        "    # That Means if Classification is Done then Find the Matching Tag.\n",
        "    if results:\n",
        "        # Long Loop to get the Result.\n",
        "        while results:\n",
        "            for i in intents['intents']:\n",
        "                # Tag Finding\n",
        "                if i['tag'] == results[0][0]:\n",
        "                    # Random Response from High Order Probabilities\n",
        "                    return print(random.choice(i['responses']))\n",
        "\n",
        "            results.pop(0)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR_THRESHOLD = 0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlEpJid-wqLv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8888147-fa56-4067-b362-6913d8389f49"
      },
      "source": [
        "while True:\n",
        "    input_data = input(\"You- \")\n",
        "    answer = response(input_data)\n",
        "    answer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You- বাসের ধাক্কায় নিহত\n",
            "রাজধানীর মতিঝিলের শাপলা চত্বর এলাকায় আজ বৃহস্পতিবার সন্ধ্যায় যাত্রীবাহী বাসের ধাক্কায় রঞ্জন দেবনাথ (৩৯) নামের এক ব্যক্তি নিহত হয়েছেন।নিহত রঞ্জন দেবনাথের স্ত্রী বীণা বিশ্বাস ঢাকা মেডিকেল কলেজ হাসপাতালের স্টাফ নার্স। তিনি জানান, রঞ্জন দেবনাথ আফতাব গ্রুপে চাকরি করেন। \n",
            "You- শনিবারের মধ্যে অবৈধ বিলবোর্ড সরাতে\n",
            "চট্টগ্রাম নগরে স্থাপিত অবৈধ সব বিলবোর্ড আগামী শনিবারের মধ্যে অপসারণ করতে বলা হয়েছে। অন্যথায় পরের দিন থেকে বিলবোর্ড স্থাপনকারী মালিকদের গ্রেপ্তার করে আইনি ব্যবস্থা নেওয়া হবে বলে মহানগর পুলিশের পাঠানো এক বিজ্ঞপ্তিতে জানানো হয়েছে।\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8imlY-v8Vce"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}